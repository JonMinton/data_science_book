# Initial Data Tidying and exploration {#tidy_data}

## Introduction 

>'Can you do Addition?' the White Queen asked. 'What's one and one and one and one and one and one and one and one and one and one?'
>'I don't know,' said Alice. 'I lost count.'

[REF TO THROUGH THE LOOKING GLASS]


Computer's operate with linear streams of discrete values. People don't. One of the first challenges in data science is therefore understanding how computers process digital data, when they are reading the data, working with the data, and writing out the data. This chapter will provide this understanding, in order to make sure you use approaches for writing and reading data that are appropriate for the tasks. 



## Eight ways of dividing file and data types 

The purpose of this section is to introduce some key distinctions relating to types of files and types of data. 

Within this section a number of code examples are provided as illustrations of the concepts being discussed. These examples make use of packages and techniques that will be formally introduced later in this book. For now, just pay attention to the key ideas, inputs and outputs, and do not be concerned about how the code 'works'. Once you have read this book and related material, consider revisiting this section with a focus on the code itself. 

### Division One: Text files and binary files 

There are many types of files on computers, usually identified with different file extensions, such as `.zip`, `.csv`, `.exe`, `.doc`, `.pdf`, and so on. Such files can be categorised into two groups:

* Text files
* Binary files 

A text file is a file that, when opened with a basic text editor, like Notepad on Windows or TextEdit on OSX, will tend to display strings of characters that are human readable. A poem or short note written in Notepad would be an example of this, but so are other types of file that, though not written so as to be read by people (at least not for fun or enlightenment) nevertheless clearly contain text when displayed in a text editor. 

As an example of a text file written in a plain text format, we can look to Project Gutenberg [REF], an online repository of classic literature that's old enough to be copyright free. This contains plain text copies of many books, as well as an application programming interface (API) for loading the data directly called `gutenbergr`[REF]. 

For example, a plain text version of Lewis Carroll's *Through the Looking Glass* is available [here](https://www.gutenberg.org/files/12/12.txt). We can read the first few lines of this file directly into R using the `readLines()` function. 


```{r}
readLines("https://www.gutenberg.org/files/12/12.txt", n = 10)

```

We can also access the same data through the `gutenbergr` API, as follows:

```{r looking_glass}
require(tidyverse)
require(gutenbergr)
gutenberg_metadata %>% 
  filter(title == "Through the Looking-Glass")

looking_glass <- gutenberg_download(12)

looking_glass %>% select(text) %>% slice(1:20)

```

By contrast, binary files are all other file types. The term 'binary' refers to the fact that computers work with zeros and ones. Binary files are those that don't clearly or obviously display as text when you look at them through a basic text editor. Instead the computer needs something more specialised for reading and interpreting that particular file type in order to make sense of it. 

The `readLines` function tries to load and display the contents of any file, assuming it is a text file, even when it is not. For example, if we try to read the first five 'lines' of an .mp3 file of a song by the Norwegian electronic music duo Royksopp, we get the following:


```{r read_rproj_as_lines}
readLines("chapter_support/05/03 49 Percent.mp3", n = 5)

```

(Note: the above text still does not make any sense even if you read Norwegian!)

### Division two: Rectangular and non-rectangular data

A distinction between types of data is between *rectangular* and *non-rectangular* data. To some extent, the difference between these two types of data should be obvious: a rectangular data file contains data that the computer expects to be arranged in some kind of rectangular table. Non-rectangular data is any other type of data. 

#### Rectangular Data

An example of rectangular data, available from the data-sharing website `figshare`, is shown below. The data reports the body mass index (BMIs) of participants in a study recorded at different time points. In the example below, the first five lines of the data are first loaded with the `readLines` function we used earlier, then using a special function, `read_csv` designed to work with rectangular data through an understanding of how the stream of data inside the file is split into different columns and rows:

```{r text_rectangle_example}
readLines("https://ndownloader.figshare.com/files/10905845", n = 5)
read_csv("https://ndownloader.figshare.com/files/10905845", n_max = 5)

```

We can see in the two examples above that, though the `readLines` function has split the above data into different rows, the contents within each row are stuck together, separated by the `,` symbol. The `read_csv` function, however, has split these values into separate columns, producing the rectangle of data that we largely expect. The `read_csv` output still has some problems, which we will fix later, but it's more suitable for this type of data. 

In order to understand how R manages to convert a text file, which is really just a stream of values, into a rectangle, in which values are neatly arranged into rows and columns, it can be helpful to think of the text file as a long length of ticker tape, and functions like `read_csv()` as a robot with a rulebook and a pair of scissors. The ticker tape contains a long series of symbols, and the rulebook gives instructions about how to cut up the ticker tape, and arrange the pieces. 

To get an even clearer idea about what this ticker tape contains, the function `readChar()` can be used rather than `readLines` above. `readChar()` carries no scissors, and does not even start to cut up the input into separate pieces. The first 200 characters in the data above are as follows:

```{r}
readChar("https://ndownloader.figshare.com/files/10905845", nchars = 200)

```

Just as with `readLines()`, the individual values within each row are separated from each other using the `,` symbol. Unlike the output from `readLines()`, however, something else is visible at regular intervals within the stream. In the output above this is represented as follows: `\n`. Although this looks like two characters, it's actually just a single character, a special character type known as the 'new line' or 'line feed' character. This character is usually invisible, and meant as an instruction for a computer program (like Word or Notepad) to separate out the contents of a text file onto different lines.  

By comparing the output from `readChar()` and `readLines()`, we can see that `readLines()` cuts the input from the file into separate outputs whenever it encouters the `\n` character. Each of the pieces is then laid out to the user in the sequence it was encountered and cut. `readLines()` rulebook is simple: cut the data at the `\n` symbol.

The `read_csv()` rule-book is a bit more complicated, but follows the same principle of cutting at specific symbols. It follows something like the following system of rules:

* First: Cut the stream into discrete chunks separated by the `\n` symbol. 
* Second: *Within each of these discrete chunks*, cut the chunk again whenever the `,` symbol is encountered. 

The first of these rules thus creates each of the rows for the dataset, and the second of these rules then creates a series of cells inside each row. In order for these cells to form a rectangular dataset, there must be the same number of cells in each row, because the number of columns in the dataset must be the same for all rows. An additional rule applied by `read_csv()` and related functions is to produce either a warning or an error if the number of commas within each of the lines is not equal.

#### Non-rectangular data

It's likely that you'll be working with rectangular data most of the time. However it's useful to be aware of what non-rectangular data can look like, and to think about the cases where non-rectangular data structures are most appropriate. Much of the internet is built using structured non-rectangular data written as text files. An simple example of this is the following:

```{r json_example}
require(jsonlite)

readLines("chapter_support/05/simple_json.json")

read_json("chapter_support/05/simple_json.json")

```

Non-rectangular data structures are hierarchical and tree-like. This ordered structure can be shown more clearly using the `Hmisc::list.tree()` function, below:


```{r vis_json_tree}

json_dta <- read_json("chapter_support/05/simple_json.json")

json_dta %>% Hmisc::list.tree()

```

In the above, the data structure has a single 'root', which splits into two branches, `edibles` and `inedibles`. The `edibles` branch itself has two branches, `fruit` and `vegetables`, which have four and three sub-branches respectively. At the end of the sub-branches are the data objects themselves themselves: four fruits, and three vegetables. The `inexibles` branch is less deep, as indicated by the number of `.` marks on the left of each line, and leads directly to four objects at the same level. 

> Activity: graphically draw the above tree. 


#### Rectangular representation of the above

The same information can often be represented using either rectangular or non-rectangular data structures. For example, a rectangular representation of the above data is as follows: 

```{r rect_rep}
rect_rep <- 
  tribble(
    ~edi_class, ~fruit_or_veg, ~name,
    "edibles",   "fruit",       "apples",
    "edibles",   "fruit",       "oranges",
    "edibles",   "fruit",       "tomatoes",
    "edibles",   "fruit",       "lemons",
    "edibles",   "vegetables",  "potatoes",
    "edibles",   "vegetables",  "sweet potatoes",
    "edibles",   "vegetables",  "aubergines",
    "inedibles", NA,            "tables",
    "inedibles", NA,            "benches", 
    "inedibles", NA,            "chairs",
    "inedibles", NA,            "pots"
)

print(rect_rep, n=20)

```

Note in the above the use of quotation marks `\"` for the contents of the cells in the rectangles. This tells R that the expected content of the cell should be a character string, and that this character string will be created within the code itself, rather than fetched from somewhere else. 

Note also the use of `NA`. This means "not applicable", and tells R that the cells being referred to should be left 'empty' rather than filled with a value. In the above example, `NA` is used to indicate that it is not appropriate to try to categorise pots, benches, chairs and other inedibles as either a fruit or a vegetable. Often, `NA` is also used to indicate that data are missing, rather than that the fields (broadly synonymous with columns when working with rectangular data) are inappropriate.   

## Data and metadata 

Another useful distinction when considering the contents of a file is between 'data', i.e. the values on which various forms of quantitative analyses will be performed, and metadata, meaning additional information relating to the data. The distinction is not always hard-and-fast, as information that could be metadata in one file could be provided as data in another, but often important to bear in mind. 

### Data with informal metadata: NorthWestern Medicine BMI data


As a simple example of the data/metadata distinction, let's return to the data downloaded from `figshare` earlier. A direct link to the data itself was used earlier, but to make sense of the data, additional information is needed, which is available from [this URL](https://figshare.com/articles/Northwestern_Medicine_BMI_data/6059513). When this website is looked at through a web browser, we can see both a preview of the contents of the data which we downloaded earlier, and below that some further information:

> *Northwestern Medicine BMI data*

>Dataset posted on 21.12.2017, 12:49 by John C. Lang Hans De Sterck Daniel M. Abrams

>Northwestern Medicine (NU) data are stored in NU.csv comma separated values (CSV) format. This file contains five columns: year t, BMI in year t, BMI in year t + 1, age in year t, and gender. Note: When BMI in year t + 1 is unavailable then the entry in the third column is -1.

Many computer scientists and information specialists may baulk calling the above description 'metadata', because this additional information is not stored in a way meant for machines to easily understand. However it is 'metadata' in the sense of being additional information about the dataset itself, and information that we require in order to allow R to work with it in the most appropriate way. Our tasks are therefore to interpret and infer the above information correctly, and produce formal (machine-interpretable) metadata that R can understand and respond to appropriately. 

We can think of the contents of the paragraph at the bottom as containing the 'metadata' we need to use the data appropriately. We know from this three new things:

1. The column names are not really the names of the names of the first row. 
2. One of the columns indicates a year, two indicate BMI values, one indicates age, and one indicates gender 
3. The value `-1` has a special meaning. It means the data are missing for that participant in that year. 

With the above (informal or inferred) metadata, we can reload and re-format the data in a more appropriate way. 
### From informal 'metadata' to formal metadata

We can start by giving appropriate names to each column. This means that the values of the first row in the dataset are not used (incorrectly) as the column names. 

```{r reload_bmi_data, cache = T}
bmi_dta <-
  read_csv(file = "https://ndownloader.figshare.com/files/10905845", 
         col_names = c("year", "bmi_t0", "bmi_t1", "age_t0", "gender")
         )

```

Next we can change the `-1` values to `NA`, R's way of representing missing data. 

```{r}
bmi_dta <- 
  bmi_dta %>% 
  mutate_each(funs(ifelse(. == -1, NA, .)))

bmi_dta

```

Notice that the second column, which used to be `-1`, is now listed correctly as `NA`, i.e. missing. 

In the above example we used the term `metadata` to refer to information that we inferred by reading some text meant for people to read. Often `metadata` is used in a more formal way, to refer to data about data written for machines to understand. 

For example, the gender column currently contains `1`s and `0`s. We know that it refers to two distinct categories of participant, male and female, but R does not. R would have to be told explicitly about the categorical quality of the values in this column, as otherwise it may process the data in inappropriate ways. For instance, if we try to summarise the contents of this column, we are likely to be interested in the numbers of males and females in the dataset. However, R might expect us to be interested in the average value of all rows in the column.


```{r}
summary(bmi_dta)

```

In the above, the summary for the gender colunn gives a mean of `0.3692`, indicating that around 37% of the participants are of the gender indicated by the value `1`. However, a more appropriate way of summarising this column might be to give a tally of each category of participants. 

We can get a sense of the kind of meta-data R has attached to each column by looking back at the table output when we printed `bmi_dta` above. Immediately below the column names is a line containing either `<int>` or `<dbl>`. `<int>` means 'integer', and means that only whole numbers (`0`, `1`, `2`, `3`, etc.) are accepted in that column, and `<dbl>` means 'double', which means that non-integer numeric values, like `2.15`, are also acceptable as inputs. 

We can change the metadata associated with the `gender` column to indicate a different data type, suitable for this type of information, in the following way

```{r}
bmi_dta %>% 
  mutate(gender = factor(gender))
```

The column `gender` now has the data type indicator `<fctr>` just beneath it, suggesting it is now a factor. We can now see how the `summary()` function works differently with this data type than the earlier, integer data type.


```{r}
bmi_dta %>% 
  mutate(gender = factor(gender)) %>% 
  summary()

```

The `summary()` function now summarises the gender column in a different way, with a tabulation by group rather than mean of the values. 


#### The ghost in the machine: making an educated guess about gender labels

One point we should note in the above is that we still do not know, from the information provided, which label (0 or 1) refers to which gender. Does a `1` indicate males, or females? 

Frustratingly, this information is not clearly stated in the [associated article](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0189795) or appendices either. Because of this, we may have to look at the associated code in order to guage this basic fact with certainty, though we can probably guess which code refers to which gender using some additional domain knowledge:

* Females live longer than males, so *if the hospital sample data are representative* of the whole US population, we might expect a higher average age for females than males. We might also expect the upper end of the distribution, such as the upper quantile (the age at which 25% of the sample are older and 75% are younger) to be higher too.
* Females tend to have somewhat higher variance in BMIs than males. 

Let's try to infer gender using this additional information, by summarising some key statistics for various columns by gender.

```{r}
bmi_dta %>% 
  group_by(gender) %>% 
  summarise(
    mean_bmi = mean(bmi_t0, na.rm = T),
    var_bmi  = mean(bmi_t0, na.rm = T),
    mean_age = mean(age_t0, na.rm = T),
    uq_age   = quantile(age_t0, probs = 0.75, na.rm = T)
  )
  
```

Based on this, we might feel more confident assuming that a `1` in the gender column indicates females, and so a `0` indicates males. Depending on how sure we need to be about this, we might either decide to go with these coding assumptions, or to seek further information to (dis)confirm this suspicion. 

For now, let's recode the gender according to our current best assumptions: 

```{r}
bmi_dta <- 
  bmi_dta %>% 
    mutate(gender = case_when(
      gender == 0 ~ 'male', 
      gender == 1 ~ 'female'
      ) %>% factor()
    ) 

bmi_dta

summary(bmi_dta)

```

The `gender` column now contains data of class `factor` (`<fctr>` in the above). We can see from the summary that the male number of rows per category is as before. 

Factors in R contain an additional type of metadata. Rather than repeatedly writing the word 'male' and 'female' in each row, R represents each label by a number, and contains a look-up table linking each number to each label. We can start to learn more about how R does this by typing the following:

```{r}
head(bmi_dta$gender)
head(as.numeric(bmi_dta$gender))
levels(bmi_dta$gender)

```

The first of these commands shows the first five items in the column: two males then three females. The second shows the numeric values actually stored, and the last function shows the labels associated each of these successive values (1, 2). Turning the gender column into a factor has created two additional types of metadata: a new identifier for the class of the data in the column (a `factor` rather than `integer`); and a value-label look-up table like the following:

| value  |  label | 
| -----  |  ------|
|   1    | female | 
|   2    | male   | 

If the dataset we have been working on in R were saved as a standard text file, much of this metadata would be lost, and have to be supplied elsewhere. But instead if it were saved in R's own binary file format, `.rds`, much of it would be retained. 

There are thus some important trade-offs to bear in mind when deciding what type of file to use to store data. 

| Data Type  | Advantages | Disadvantages | 
| ---------- + ---------- + ------------- |
| Text       |  
<ul>
  <li> More interoperable: Works with many programs
  <li> Likely to remain readable across time
</ul>
| 
<ul>
  <li> Metadata lost 
  <li> Sometimes produces larger file sizes
</ul>
| Binary     | 
<ul>
  <li> Data and metadata in same file
  <li> Sometimes file sizes are smaller, faster to read etc
</ul>
|
<ul>
  <li> Less interoperable
</ul>
| 

* Note to self: fix the above



## Tidy and untidy data 

A final way of distinguishing between rectangular data is between *tidy data* and *untidy data*. This distinction, like many recent developments in the use of R for data science, was popularised by Hadley Wickham, via a [2014 article](https://www.jstatsoft.org/article/view/v059i10/v59i10.pdf) and associated software. 

Wickham defines data as tidy if the following is true:

1. Each variable forms a column
2. Each observation forms a row
3. Each type of observational unit forms a table

As is often the case, tidy data is often easiest to understand through examples, rather than purely through definition. One of the first examples given in the [2014 article](https://www.jstatsoft.org/article/view/v059i10/v59i10.pdf) concerns the following table:

| row | a | b | c |
| --- | - | - | - |
| A   | 1 | 4 | 7 | 
| B   | 2 | 5 | 8 | 
| C   | 3 | 6 | 9 |

A tidy data version of the above looks as follows:

| row | column | value |
| --- | ------ | ----- |
|  A  | a      | 1     | 
|  B  | a      | 2     | 
|  C  | a      | 3     | 
|  A  | b      | 4     | 
|  B  | b      | 5     | 
|  C  | b      | 6     | 
|  A  | c      | 7     | 
|  B  | c      | 8     | 
|  C  | c      | 9     | 

My own way of thinking about tidy data is to think about the columns in a dataset as being of two fundamentally different types: 

* Where/locator variables
* What/measurement variables

For the third of the criteria above to be true, "Each type of observational unit forms a table", each row must have a unique combination of locator variables. In the table above, the columns marked `row` and `column` are the locator variables, and the column marked `value` is the measurement variables. 

For example, imagine that, instead of the 3-by-3 table above, we had a table giving height above sea level (in km) for various combinations of latitude and longitude:

| latitude | 10 | 20 | 30 |
| --- | - | - | - |
| 10   | 1 | 4 | 7 | 
| 20   | 2 | 5 | 8 | 
| 30   | 3 | 6 | 9 |

The tidy data version of this table now looks as follows:

| latitude | longitude | elevation |
| --- | ------ | ----- |
|  10  | 10      | 1     | 
|  20  | 10      | 2     | 
|  30  | 10      | 3     | 
|  10  | 20      | 4     | 
|  20  | 20      | 5     | 
|  30  | 20      | 6     | 
|  10  | 30      | 7     | 
|  20  | 30      | 8     | 
|  30  | 30      | 9     | 

It's clear in this example that elevation is *what is measured*, and latitude and longitude are needed to define *where it is measured*. Both latitude and longitude need to be specified in order for the measurements to be 'labelled' correctly. Similarly, we could imagine additional measurements, like average annual rainfall, having been taken at the same locations, which would be recorded in the table as an additional *what* column:

| latitude | longitude | elevation | rainfall | 
| --- | ------ | ----- | ----------| 
|  10  | 10      | 1     | 15 | 
|  20  | 10      | 2     | 20 | 
|  30  | 10      | 3     | 10 |
|  10  | 20      | 4     | 40 |
|  20  | 20      | 5     | 10 | 
|  30  | 20      | 6     | 5  | 
|  10  | 30      | 7     | 45 | 
|  20  | 30      | 8     | 35 |
|  30  | 30      | 9     | 20 |

Once additional columns are added, the advantages of the tidy data arrangement starts to  become clearer. If the original table structure were used, with each row a different latitude and each column a different longitude, then two separate tables would have to have been produced. With the tidy data arrangement, the *where* columns define the table's observational unit, and so long as measurements pertain to the table's observational unit, then they can be added as additional *what* columns in the dataset, as above. 

In order to think further about what's meant by an observational unit, let's return to the example of rainfall. In the above example, the `rainfall` column is the annual average rainfall. But of course not all years are alike. We could instead imagine a new table, which includes `latitude`, `longitude` and `year` as *where* variables, and `rainfall` as the only *what* variable:

| latitude | longitude | year | rainfall | 
| --- | ------ | ----- | ----------| 
|  10  | 10      | 2000     | 15 | 
|  20  | 10      | 2000     | 20 | 
|  30  | 10      | 2000     | 10 |
|  10  | 10      | 2010     | 18 | 
|  20  | 10      | 2010     | 22 | 
|  30  | 10      | 2010     | 9 |

Why would `elevation` not be included in the above table, with its latitude-longitude-year observational unit? Because, barring an earthquake, volcanic eruption, or Dutch engineering, `elevation` is likely to remain constant over each of the observed years. For elevation, the differences (in terms of years) do not make a difference to the measured values. The appropriate observational unit for `elevation` is `latitude-longitude`, not `latitude-longitude-year`. 

## Loading and saving data - R packages and functions

Part (b) will discuss some of the particular R packages and functions within that can be used to work with different file types, presenting even the loading of data as at iterative process of learning more about how variables in data are structured and saved, and how the information in these variables be best represented and operated with inside R. 

## Data tidying - principles and challenges 

Part (c) will introduce a further, higher level, distinction between types of data: from ‘untidy’ and ‘initial’ data to ‘tidy’ and ‘derived’ data. This section will frame much of the practical challenge of data science as involving first identifying what needs to be done to move from the former to the latter, and then knowing how to do this.

## Data tidying - tidyr and dplyr

Part (d) will introduce two closely linked R packages, tidyr and dplyr, as providing the tools necessary to complete the vast majority of data tidying and data derivation tasks.

## Initial Exploratory data analysis

Finally, part (e) will emphasise the importance of rapid exploratory data analysis both at the data tidying stage, and for developing familiarity and engagement with tidied data sources.
