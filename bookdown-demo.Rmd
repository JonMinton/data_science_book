--- 
title: "A DS Book Example"
author: "Jon Minton"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: jonminton/data_science_book
description: "This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook."
---

# Prerequisites

This is a _sample_ book written in **Markdown**. You can use anything that Pandoc's Markdown supports, e.g., a math equation $a^2 + b^2 = c^2$.

The **bookdown** package can be installed from CRAN or Github:

```{r eval=FALSE}
install.packages("bookdown")
# or the development version
#devtools::install_github("rstudio/bookdown")
```

Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading `#`.

To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): <https://yihui.name/tinytex/>.

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

# Introduction {#intro}

You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \@ref(methods).

Figures and tables with captions will be placed in `figure` and `table` environments, respectively.

```{r nice-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'}
par(mar = c(4, 4, .1, .1))
plot(pressure, type = 'b', pch = 19)
```

Reference a figure by its code chunk label with the `fig:` prefix, e.g., see Figure \@ref(fig:nice-fig). Similarly, you can reference tables generated from `knitr::kable()`, e.g., see Table \@ref(tab:nice-tab).

```{r nice-tab, tidy=FALSE}
knitr::kable(
  head(iris, 20), caption = 'Here is a nice table!',
  booktabs = TRUE
)
```

You can write citations, too. For example, we are using the **bookdown** package [@R-bookdown] in this sample book, which was built on top of R Markdown and **knitr** [@xie2015].

<!--chapter:end:01-intro.Rmd-->

# Literature

Here is a review of existing methods.

<!--chapter:end:02-literature.Rmd-->

# Methods

We describe our methods in this chapter.

<!--chapter:end:03-method.Rmd-->

# Applications

Some _significant_ applications are demonstrated in this chapter.

## Example one

## Example two

<!--chapter:end:04-application.Rmd-->

# Initial Data Tidying and exploration {#tidy_data}

## Introduction 

>'Can you do Addition?' the White Queen asked. 'What's one and one and one and one and one and one and one and one and one and one?'
>'I don't know,' said Alice. 'I lost count.'

[REF TO THROUGH THE LOOKING GLASS]


Computer's operate with linear streams of discrete values. People don't. One of the first challenges in data science is therefore understanding how computers process digital data, when they are reading the data, working with the data, and writing out the data. This chapter will provide this understanding, in order to make sure you use approaches for writing and reading data that are appropriate for the tasks. 



## Eight ways of dividing file and data types 

There are many types of files on computers, usually identified with different file extensions, such as `.zip`, `.csv`, `.exe`, `.doc`, `.pdf`, and so on. Such files can be categorised into two groups:

* Text files
* Binary files 

A text file is a file that, when opened with a basic text editor, like Notepad on Windows or TextEdit on OSX, will tend to display strings of characters that are human readable. A poem or short note written in Notepad would be an example of this, but so are other types of file that, though not written so as to be read by people (at least not for fun or enlightenment) nevertheless clearly contain text when displayed in a text editor. 

As an example of a text file written in a plain text format, we can look to Project Gutenberg [REF], an online repository of classic literature that's old enough to be copyright free. This is accessible within R through a package called `gutenbergr`[REF]. With a few lines of code we can download and jump right into some plain text data clearly meant for humans rather than machines: 

```{r looking_glass}
require(tidyverse)
require(gutenbergr)
gutenberg_metadata %>% 
  filter(title == "Through the Looking-Glass")

looking_glass <- gutenberg_download(12)

looking_glass[100:105,"text"]

```
By contrast, binary files are all other file types. The term 'binary' refers to the fact that computers work with zeros and ones. Binary files are those that don't clearly or obviously display as text when you look at them through a basic text editor. Instead the computer needs something more specialised to the type of data in order to make sense of it. 

The `readLines` function tries to load and display the contents of any file, assuming it is a text file, even when it is not. For example, if we try to read the first five 'lines' of an .mp3 file of a song by the Norwegian electronic music duo Royksopp, we get the following:


```{r read_rproj_as_lines}
readLines("chapter_support/05/03 49 Percent.mp3", n = 5)

```

(Note: the above text still does not make any sense even if you read Norwegian!)

A second way of distinguishing between types of files, more relevant when thinking about data files, is the following:

* Rectangular data files
* Non-rectangular data files 

To some extent, the difference between these two types of data should be obvious: a rectangular data file contains data that the computer expects to be arranged in some kind of rectangular table. Non-rectangular data is any other type of data. 

An example of rectangular data, available from the data-sharing website `figshare`, is shown below. The data reports the body mass index (BMIs) of participants in a study recorded at different time points. In the example below, the first five lines of the data are first loaded with the `readLines` function we used earlier, then using a special function, `read_csv` designed to work with rectangular data through an understanding of how the stream of data inside the file is split into different columns and rows:

```{r text_rectangle_example}
readLines("https://ndownloader.figshare.com/files/10905845", n = 5)
read_csv("https://ndownloader.figshare.com/files/10905845", n_max = 5)

```

We can see in the two examples above that, though the `readLines` function has split the above data into different rows, the contents within each row are stuck together, separated by the `,` symbol. The `read_csv` function, however, has split these values into separate columns, producing the rectangle of data that we largely expect. The `read_csv` output still has some problems, which we will fix later, but it's more suitable for this type of data. 


It's likely that you'll be working with rectangular data most of the time. However it's useful to be aware of what non-rectangular data can look like, and to think about the cases where non-rectangular data structures are most appropriate. Much of the internet is built using structured non-rectangular data written as text files. An simple example of this is the following:

```{r json_example}
require(jsonlite)

readLines("chapter_support/05/simple_json.json")

read_json("chapter_support/05/simple_json.json")

```

Non-rectangular data structures are hierarchical and tree-like. For example, the structure of the above can be shown graphically as follows:


```{r vis_json_tree}

json_dta <- read_json("chapter_support/05/simple_json.json")
json_dta %>% data.tree::as.Node() 

```


Part (a) will introduce the primary distinction used by computer programmers between types of files, between human-readable text files and human-unreadable binary files. Examples of these two different types of files will be introduced, which also highlight another important conceptual distinction: the difference between data and meta-data (data about data). The relative advantages of different file types will be discussed, and the argument will be made that, as a rule-of-thumb, text files should be used in preference to binary files.

## Loading and saving data - R packages and functions

Part (b) will discuss some of the particular R packages and functions within that can be used to work with different file types, presenting even the loading of data as at iterative process of learning more about how variables in data are structured and saved, and how the information in these variables be best represented and operated with inside R. 

## Data tidying - principles and challenges 

Part (c) will introduce a further, higher level, distinction between types of data: from ‘untidy’ and ‘initial’ data to ‘tidy’ and ‘derived’ data. This section will frame much of the practical challenge of data science as involving first identifying what needs to be done to move from the former to the latter, and then knowing how to do this.

## Data tidying - tidyr and dplyr

Part (d) will introduce two closely linked R packages, tidyr and dplyr, as providing the tools necessary to complete the vast majority of data tidying and data derivation tasks.

## Initial Exploratory data analysis

Finally, part (e) will emphasise the importance of rapid exploratory data analysis both at the data tidying stage, and for developing familiarity and engagement with tidied data sources.

<!--chapter:end:05-tidy_data.Rmd-->

#	Data Visualisation {#data_vis}

## What is data visualisation? The grammar of graphics approach

Part (a) will introduce Leland Wilkinson’s ‘grammar of graphics’ philosophy to thinking about 
data visualisation, and for producing clear modular instructions for the production of data graphics. 


## Exploratory data analysis using ggplot2
Part (b) will describe how the concept of the ‘grammar of graphics’ has been operationalised within R 
within the hugely popular ggplot2 package, and more recently ggvis, and present some extended examples 
of their use in exploring the tidied dataset introduced previously. 


## Producing tables in R 

Part (c) will cover the considerations and practicalities of table production, both in the context 
of exploratory data analysis, and when producing final outputs for papers and reports. 
The relative merits of figures and tables – both in principle and in practice – will also be discussed at this stage. 


## Producing publication-ready graphs

Part (d) will return to more conventional types of data visualisation, and address how to polish and 
tweak the graphs produced using ggplot2 and related packages into something publication ready. 
Issues such as how to control cosmetic features such as background styles, fonts and tick label orientation, 
will be discussed, as well as how to produce both monochrome and colour versions of images, how to add 
additional labels and captions within figures, and how to save images as files of the correct type and resolution.


## An introduction to spatial maps

Part (e) will provide an introduction to using R to produce maps. Spatial data are handled very differently 
in R to most other types of data featured in this book, and so working with such data produces a series of 
additional challenges. However, maps can be immensely powerful tools for analysis, often revealing otherwise 
hidden spatial structures to social data. Although the challenges of working with spatial data mean that 
many of the packages introduced earlier in the book are not appropriate, this part of the chapter will 
emphasise continuity with previous material on data visualisation by focusing on two packages which are
both inspired by the grammar of graphics philosophy, ggmap and tmap. 



<!--chapter:end:06-data_vis.Rmd-->

#	Text analysis {#text_explore}

This section will provide a brief introduction to using R to explore text documents, an unusual type of data often
encountered more associated with qualitative than quantitative research in the social sciences. This chapter will not provide an extensive, formal discussion of how to produce statistical models based on text, but instead should serve as a taster of some of the features of the recent `tidytext` package by XXX. 


<!--chapter:end:07-text_analysis.rmd-->

# Statistics {#stats}

Most books on quantitative methods focus very heavily on statistics. This book will instead situate statistics as an element within a much broader process of activities in the service of getting insight from data, and making better decisions on the basis of this insight. 

## The aims of statistics: data reduction or inference? 

Part (a) will argue for statistics, by which is meant primarily the production of summary statistics, as a key part of the data science toolkit from two parallel perspectives: in addition to recourse to theories of statistical inference, which have as their key concern the appropriate use of data to populate parameters in stochastic models, there are also important cognitive reasons for producing summary statistics, namely that often more data means less information, as people are severely limited both in time and memory as to the number of ‘facts’ they can hold onto and make sense of at a time. Summary statistics are therefore invaluable as tools for intelligent data reduction: data reduction because it turns many thousands or millions of observations into just a handful of key values, and intelligent because these key values are ‘less worse’ at standing in place of many more values than most alternatives. The relationship between statistical modelling and signal processing will also be discussed. 
Theories of statistical inference, and in particular likelihood theory, provide a means of selecting between many different models – each assuming different stochastic structures and processes, and hence each embodying different theories about social phenomena – and of deciding on the right balance between model accuracy and model parsimony, but only when models have a particular structural relationship to each other (When one model can be thought of as a ‘constrained’ variant of another model).

## Balancing accuracy and parsimony using AIC and BIC

Part (b) will discuss this further, and offer Aikiake Information Criterion (AIC) and the Bayesian Information Criterion (BIC) as a way of guiding the accuracy/parsimony trade-off in selecting between many types of statistical model.

## Statistical Simulation: Showing what models really mean

Having decided on the ‘least worst’ model to represent the data, part (c) will describe a series of methods and functions – including sim within the arm package and the Zelig package – for extracting substantive meaning from statistical models, going beyond the usual convention of stopping and starting with the question of whether particular model parameters are ‘statistically significant’ or not, to instead making predictions and projections about the expected and predicted effects of change within social systems. 

## Advanced statistics: structural modelling to formalise informal knowledge
  
Part (d) will discuss further how statistical models can be developed to formalise implicit and ‘fuzzy’ knowledge and expectations about social relationships, discussing both the dangers of going ‘too far’ and developing such complicated and precisely structured models that it becomes easy to confuse ‘the map for the territory’, and the danger of not going far enough and failing to capture important structure and shape in the relationships between variables and social factors. 

<!--chapter:end:08-statistics.rmd-->

# Programming and Automation {#Coding}

A guiding principle in this book is the following: more emphasis should be given to training students about technical issues in quantitative methods courses, because technical issues are not interesting. This is because often ‘technical issues’, such as ensuring software are loaded correctly and searching for bugs and syntax errors in code, can take up a large proportion of the time available to social researchers to do research; if the definition of ‘technical issues’ were expanded to include many routine ‘janitorial’ data management issues like reformatting data and managing different versions of code and documents, then ‘technical issues’ usually take the majority of researchers’ time. The rationale for focusing more on teaching methods and approaches for handling technical issues is therefore that if even small efficiency gains can be found in some of these areas then much more time will be freed to perform more interesting and substantively meaningful tasks, such as exploring data and thinking carefully about interpretations, implications and social hypotheses. 

R is a statistical programming language rather than simply a statistical package. What this means is that R has a steeper learning curve initially than a statistical package, but that both the range and efficiency of data science tasks which can be performed in R can be much greater than for a standard statistical software package. However, in order to make use of the benefits of R, it needs to be approached as a programming language, and in order to do this, something about programming languages needs to be covered within this book. This chapter will provide this necessary background knowledge required to make best use of R in data science, and in particular for using its functionality to greatly cut down the amount of time and effort required to solve routine technical and janitorial data science tasks. 

##	Writing user-defined functions

Functions will have been used throughout the book up until this point. Part (a) formally introduces them in R, providing explicit guidance on what functions are, how they work, and how they can be used to speed up, standardise, and reduce the scope for errors to accumulate within the data-to-value chain. Functions provide building blocks for automation, and automation means that the marginal costs of doing more, in terms of researcher time and effort, fall to almost nothing, and so more data analyses and explorations can be performed. 


##	Automating analyses using purrr

Part (b) moves from the individual function, to functional programming philosophies more generally, and shows how this broader understanding of functional programming allow for automation of tasks at each and every stage of the data-to-value process. The purrr package, effectively a series of functions for working with functions, will be introduced at this stage; purrr functions integrate with user-defined functions to allow the same complex processes and procedures to be applied many times to many different pieces or chunks of data. 

##	Automating data tidying

Parts (c), (d) and (e) will provide examples of purrr in use for a range of different activities: automating the loading, tidying and arranging of data from many different files; automating the production of graphs and summary findings from many different sections of a dataset, producing dozens or hundreds of files at a time; and automating either the running of a single statistical model to multiple pieces of data, or alternatively running many different types of model to a single dataset

##	Automating graphs and tables

Parts (c), (d) and (e) will provide examples of purrr in use for a range of different activities: automating the loading, tidying and arranging of data from many different files; automating the production of graphs and summary findings from many different sections of a dataset, producing dozens or hundreds of files at a time; and automating either the running of a single statistical model to multiple pieces of data, or alternatively running many different types of model to a single dataset


##	Automating statistical analyses 

Parts (c), (d) and (e) will provide examples of purrr in use for a range of different activities: automating the loading, tidying and arranging of data from many different files; automating the production of graphs and summary findings from many different sections of a dataset, producing dozens or hundreds of files at a time; and automating either the running of a single statistical model to multiple pieces of data, or alternatively running many different types of model to a single dataset



. 

<!--chapter:end:09-coding.Rmd-->

---
output:
  pdf_document: default
  html_document: default
---
# Open, Reproducible and Collaborative Research {#repro_research}

A vitally important aspect of any science is to be able to produce claims that can be tested and verified by other people. As research activities are tending to become more complex - conceptually, methodologically and technically – there need to work collaboratively has become increasingly important. 

## The theory and challenges of open and reproducible research

Part (a) will consider both the benefits and the barriers to open and reproducible research in the context of the social and health sciences, where both the nature of the data and the background of researchers tend to be very different to disciplines like physics, where open and reproducible research practices tend to be more commonplace. 


## Sharing results and findings using github

Part (b) will turn to the practical means and benefits of using GitHub, integrated with Rstudio projects, for making both the code and processes produced available to your future self, but also tidied and derived data, and initial findings and figures, available to other researchers. 

## Writing collaboratively with wikis, markdown and related approaches

Part (c) will discuss linked features and programs that allow draft reports and other written material to be shared and co-developed with other researchers. 

## Managing literature (Optional)

Finally part (d) covers approaches to managing literature and literature review in the context of R projects and the research-project-as-suitcase philosophy advocated throughout the book.  

<!--chapter:end:10_repro_research.rmd-->

`r if (knitr:::is_html_output()) '
# References {-}
'`

<!--chapter:end:11-references.Rmd-->

